{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT PROCESSING\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the recognized text involves working with the output of the speech recognition step, which is the transcribed text obtained from the audio input. In Python, once you have the recognized text, you can perform various operations on it based on your specific requirements. Here are a few examples:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text Analysis: Analyze the recognized text using natural language processing (NLP) techniques. You can use libraries such as NLTK (Natural Language Toolkit) or spaCy to perform tasks like part-of-speech tagging, named entity recognition, sentiment analysis, or topic extraction. These analyses can provide valuable insights and help you understand the content and context of the speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\c21054458\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\c21054458\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\c21054458\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "\n",
    "recognized_text = 'Edinburgh because well lots of buses and planes go high good place and I went to the eye boggling thing'\n",
    "\n",
    "# Tokenize the recognized text into sentences and words\n",
    "sentences = sent_tokenize(recognized_text)\n",
    "words = word_tokenize(recognized_text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Perform named entity recognition\n",
    "named_entities = ne_chunk(pos_tags)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Entity Extraction: Extract specific information or entities from the recognized text. You can define patterns or use regular expressions to identify relevant entities such as names, dates, locations, or numbers. These extracted entities can then be used as parameters for your SPARQL query or for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract dates using regular expressions\n",
    "date_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n",
    "dates = re.findall(date_pattern, recognized_text)\n",
    "\n",
    "# Extract names using a pattern\n",
    "name_pattern = r'My name is (\\w+)'\n",
    "match = re.search(name_pattern, recognized_text)\n",
    "if match:\n",
    "    name = match.group(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Generation: Generate a SPARQL query based on the recognized text and the specific task or intent. You can use string manipulation techniques to construct a valid SPARQL query, incorporating the extracted entities or applying predefined templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a SPARQL query using recognized entities\n",
    "query_template = \"SELECT ?property ?value WHERE {{ <{0}> ?property ?value }}\"\n",
    "entity = \"http://example.org/entity\"\n",
    "sparql_query = query_template.format(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT ?property ?value WHERE { <http://example.org/entity> ?property ?value }'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparql_query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
